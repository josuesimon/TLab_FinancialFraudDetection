Reflection:

i. Which insights did you gain from your EDA? 
Overall from my EDA I saw that the data was heavily skewed, it made it seem as if all of the data was performing as not fraud. This made me realize that I needed to be careful when interpreting the results and that I needed to really consider what things I should be narrowing down on in order to get data that could further reveal actual fraud. 

ii. How did you determine which columns to drop or keep? If your EDA informed this process, explain which insights you used to determine which columns were not needed. 
I ended up deciding which columns to drop by analyzing the actual data and seeing which columns were not really contributing. For the two columns I dropped, I saw the information as not seeming very helpful and therefore it was dropped. 

iii. Which hyperparameter tuning strategy did you use? Grid-search or random-search? Why? 
I ended up using grid search because I was trying to find the best combination of parameters and I wanted to make sure that I was getting the best results. I also wanted to make sure that I was not missing any important combinations of parameters. It made the most sense to me due to the volume of our data.

iv. How did your model's performance change after discovering optimal hyperparameters? 
I did not get to this step as my model never finished running. It ran for over 1000 minutes and still did not finish.

v. What was your final F1 Score? 
Again, my model did not finish running so I was unable to get a final F1 score.
