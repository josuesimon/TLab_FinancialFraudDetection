{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will be importing our necessary libraries to be able to use them in our code.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns  # For confusion matrix visualization\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   step      type    amount     nameOrig  oldbalanceOrg  newbalanceOrig  \\\n",
      "0     1   PAYMENT   9839.64  C1231006815       170136.0       160296.36   \n",
      "1     1   PAYMENT   1864.28  C1666544295        21249.0        19384.72   \n",
      "2     1  TRANSFER    181.00  C1305486145          181.0            0.00   \n",
      "3     1  CASH_OUT    181.00   C840083671          181.0            0.00   \n",
      "4     1   PAYMENT  11668.14  C2048537720        41554.0        29885.86   \n",
      "\n",
      "      nameDest  isFraud  isFlaggedFraud  \n",
      "0  M1979787155        0               0  \n",
      "1  M2044282225        0               0  \n",
      "2   C553264065        1               0  \n",
      "3    C38997010        1               0  \n",
      "4  M1230701703        0               0  \n"
     ]
    }
   ],
   "source": [
    "# We will load our data from our clean_data.csv, rename it 'data' and display the first few rows of our data to\n",
    "# remind ourselves the columns that we are working with and \n",
    "data = pd.read_csv('../Data/clean_data.csv')\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will create a new data frame called X and exclude 'isFraud' and 'isFlaggedFraud' features\n",
    "# And also create the 'y' target variable to isolate it from the rest of our data set\n",
    "# This will help our  model learn to predict the target variable based on the features in our data set\n",
    "X = data.drop(columns=['isFraud', 'isFlaggedFraud'])  # Features\n",
    "y = data['isFraud']  # Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are splitting our dataset into training and testing subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are identifying numeric and categorical columns\n",
    "numeric_cols = X_train.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "categorical_cols = X_train.select_dtypes(include=['object']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will be creating a column transformer for preprocessing the data as I kept on getting errors\n",
    "# The first part of the transformer replaces any missing values with the mean of the non-missing values in that column and standardizes the features by removing the mean and scaling to unit variance. \n",
    "# The second part is similar to that of the previous step except we are now filling in any missing values with the most common one and then converting the categories into a binary format (0 and 1), making it easier for the machine learning model to process the data\n",
    "# The preprocessor then combines both the first and second step, which allows for both types of data to be prepared and used by the machine learning model\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')), \n",
    "    ('scaler', StandardScaler()) \n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  \n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore')) \n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will create a Pipeline that includes the preprocessor and the random forest classifier into one workflow called Pipeline that prepares the data and then uses it for classification\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42)) \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step first fits the pipeline to our data, makes predictions on the data using the trained model, and then evaluates the random forest classifier's performance \n",
    "# by printing a classification report and a confusion matrix.\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred_rf = pipeline.predict(X_test)\n",
    "print(\"Evaluation for Random Forest:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This next model sets up a pipeline for training a decision tree model by combining the preprocessing steps with the decision tree classifier, fits the model to the training data,\n",
    "# makes predictions on our data, and then evaluates its performance by also displaying a classification report and a confusion matrix.\n",
    "decision_tree_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', DecisionTreeClassifier(random_state=42))\n",
    "])\n",
    "decision_tree_pipeline.fit(X_train, y_train)\n",
    "y_pred_tree = decision_tree_pipeline.predict(X_test)\n",
    "print(\"Evaluation for Decision Tree:\")\n",
    "print(classification_report(y_test, y_pred_tree))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code prepares the fine-tuning of the random forest model by specifying different options for its key settings, like the number of trees and how deep each tree can grow, \n",
    "# and then using GridSearchCV, finds the best combination of the best-performing model (determining it on how well it predicts based on the F1 score).\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [100, 200, 300],\n",
    "    'classifier__max_depth': [None, 10, 20, 30],\n",
    "    'classifier__min_samples_split': [2, 5, 10],\n",
    "    'classifier__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, \n",
    "                           cv=3, n_jobs=-1, verbose=2, scoring='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step runs grid search to train the random forest model on our data, exploring different combinations of parameters and printing out the best set of parameters for optimal performance.\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"The best parameters found for Random Forest: \", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step utilizes the best version of the random forest model that was found during our tuning process and uses it to make predictions\n",
    "# on our data, allowing us to see how well the model performs on new examples\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "y_pred_best_rf = best_rf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code checks how well the best random forest model performed by calculating the F1 score and showing a detailed report of its results. \n",
    "# It also creates a confusion matrix that shows how many predictions were correct and how many were wrong.\n",
    "f1 = f1_score(y_test, y_pred_best_rf)\n",
    "print(f\"F1 Score of the best Random Forest model: {f1:.2f}\")\n",
    "print(\"Best Random Forest Model Evaluation:\")\n",
    "print(classification_report(y_test, y_pred_best_rf))\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_best_rf), annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix for Best Random Forest Model')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
